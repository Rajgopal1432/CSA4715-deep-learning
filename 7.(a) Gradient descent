import numpy as np


np.random.seed(0)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)


X_b = np.c_[np.ones((100, 1)), X]

def gradient_descent(X, y, iterations=1000, learning_rate=0.01, stopping_threshold=0.0001):
    m = len(X)
    n = X.shape[1]
    theta = np.random.randn(n, 1)  # Random initialization of parameters

    for i in range(iterations):
        gradients = 2/m * X.T.dot(X.dot(theta) - y)
        theta = theta - learning_rate * gradients
        # Calculate mean squared error as loss
        loss = np.mean((X.dot(theta) - y)**2)
        if loss < stopping_threshold:
            break

    return theta

theta = gradient_descent(X_b, y)

print("Optimal parameters:")
print(theta)
